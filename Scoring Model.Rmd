---
title: "Scoring Model"
author: "Foggetti Marco 5111158"
date: '2022-06-16'
output: pdf_document
---

```{r}
library(ggplot2)
library(lattice)
library(caret)
library(corrplot)
library(plyr)
```

```{r}
library(readxl)
X <- read_excel("Dataset_x_Scoring_Model_Completo (1).xlsx", 
     sheet = "Dataset_x_Scoring_Model")
X <- as.data.frame(X)
X$tipo_multib <- as.factor(X$tipo_multib)
dim(X)
average_churn_rate <- sum(X$flg_target)/length(X$flg_target)
average_churn_rate
```
Removing variables having high missing percentage (> 0.5)
```{r}
dat1 <- X[, colMeans(is.na(X)) <= .5]
dim(dat1)
```

Removing zero and near zero variance predictors
```{r}
nzv <- nearZeroVar(dat1)
dat2 <- dat1[, -nzv]
dim(dat2)
```

**MODEL SELECTION**
To avoid numerical problems we first remove the cases with NA, obtaining a smaller but consistent dataset
```{r}
dat_MS <- na.omit(dat2)
dim(dat_MS)
```

Identifying numeric variables
```{r}
numericData <- dat_MS[sapply(dat_MS, is.numeric)]
colnames(numericData)
```

Correlation Matrix
```{r}
descrCor <- cor(numericData)
summary(descrCor[upper.tri(descrCor)])
```
```{r}
corrplot(descrCor, order = "FPC", method = "color", type = "lower", tl.cex = 0.7, tl.col = rgb(0, 0, 0))
```
```{r}
highlyCorrelated <- findCorrelation(descrCor, cutoff=0.9)
highlyCorrelated #indexes of high correlated variables
```
```{r}
highlyCorCol <- colnames(numericData)[highlyCorrelated]
#Variable Names of Highly Correlated Variables
highlyCorCol
```
Remove highly correlated variables and create a new dataset
```{r}
dat3 <- dat_MS[, -which(colnames(dat_MS) %in% highlyCorCol)]
dim(dat3)
#we also remove the column id for the analysis
dat3 <- dat3[,-1]
dim(dat3)
```
```{r}
colnames(dat3)
```

#Trying model selection on AIC 
```{r}
#attach(X)
#failures <- numeric()
#for(i in 1:length(flg_target)){
#    if(flg_target[i] != 1) {
#      failures[i] = 1
#    }
#    else failures[i] = 0
#  }
#length(failures)
```

```{r}
#library(regclass)
#model <- glm(flg_target~. - failure, family = "binomial", data = dat3)
#build_model(model, data = dat3, type = "descriptive")
```




***Dividing dataset into train and test***
```{r}
set.seed(1)
train <- sample(1:nrow(dat3), 0.70*nrow(dat3), replace = F)

test <- dat3[-train,]
train <- dat3[train,]
```

Perform logistic regression on train dataset
```{r}
model1 <- glm(flg_target~., data = train, family = "binomial")
summary(model1)
```

```{r}
a <- drop1(model1, test = "Chisq")
a
```

***Predicted probabilities***
```{r}
pred_train <- predict(model1, type = "response")
head(pred_train)
length(pred_train)
```
```{r}
table_train <- cbind(train$flg_target, pred_train)
colnames(table_train) <- c("flg target", "Predicted Probabilities")
head(table_train)
```

```{r}
library(gains)
lift_train<- gains(actual = train$flg_target, predicted = pred_train, groups = 20
      )
lift_train
```

***Test the model***
```{r}
pred_test <- predict(model1, newdata = test, type = "response")
head(pred_test)
length(pred_test)
```

```{r}
table_test <- cbind(test$flg_target, pred_test)
colnames(table_test) <- c("flg target", "Predicted Probabilities")
head(table_test)
```

```{r}
library(gains)
lift_test <- gains(actual = test$flg_target, predicted = pred_test, groups = 20
      )
lift_test
```
We can observe that the model do not overfits the data

***We can now consider the whole dataset***
```{r}
pred_tot <- predict(model1, type = "response", newdata = dat3)
head(pred_tot)
```
```{r}
lift_tot <- gains(actual = dat3$flg_target, predicted = pred_tot, groups = 20)
lift_tot
```


```{r}
plot.gains(lift_tot, col = c(2,3,4))
```
```{r}
table_tot <- cbind(dat3$flg_target, pred_tot)
colnames(table_tot) <- c("flg target", "Predicted Probabilities")
head(table_tot)
```
```{r}
library(dplyr)
table_tot <- as.data.frame(table_tot)
order_table <- arrange(table_tot, desc(pred_tot))
head(order_table)
```
```{r}
sum(order_table$`flg target`[1:227])/227
```


